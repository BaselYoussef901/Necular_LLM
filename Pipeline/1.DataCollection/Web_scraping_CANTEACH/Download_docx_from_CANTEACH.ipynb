{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading articles by system technology from CANTEACH\n",
    "\n",
    "This Python script automates the process of scraping and downloading PDF documents from the CANTEACH website (https://canteach.candu.org/Pages/Home.aspx). It uses Selenium for web scraping and BeautifulSoup for parsing HTML content. The script is structured to handle interruptions and resume progress, ensuring efficient and complete downloads. Below is a detailed explanation of the code:\n",
    "\n",
    "### Setup and Initialization:\n",
    "\n",
    "The script imports necessary libraries for web scraping, browser automation, and file handling.\n",
    "- It defines the base URL of the CANTEACH website, the main page URL containing system links, and the local directory where the documents will be saved.\n",
    "- It ensures that the directory for saving documents exists.\n",
    "\n",
    "### Setting Up Selenium WebDriver:\n",
    "\n",
    "The script initializes the Selenium WebDriver for Firefox, with an option to run headless.\n",
    "- It clears all cookies to avoid issues with large request headers.\n",
    "\n",
    "### Getting System URLs:\n",
    "\n",
    "The script sends a request to the main page to get URLs for different systems.\n",
    "- It parses the HTML content of the page to find links that contain \"Forms\" in their href attribute, as these are the links to the system pages.\n",
    "\n",
    "### Scraping Document Links:\n",
    "\n",
    "The script uses Selenium to navigate each system page and collect PDF document links.\n",
    "- It opens each system URL, clears cookies again, and waits for the page to load completely.\n",
    "- It locates elements containing the text 'Title', which, when expanded, reveal the PDF links.\n",
    "- It scrolls each 'Title' element into view and clicks on it to expand the section, then collects all PDF links from the expanded sections.\n",
    "\n",
    "### Downloading Documents:\n",
    "\n",
    "The script downloads each PDF document and saves it to the specified folder.\n",
    "- It handles exceptions to ensure the script continues running even if a download fails.\n",
    "\n",
    "### Saving and Loading Progress:\n",
    "\n",
    "- The script saves the progress of downloaded URLs to a file, allowing it to resume from where it left off if interrupted.\n",
    "- It loads the progress from the file upon restart.\n",
    "\n",
    "### Main Function:\n",
    "\n",
    "The main function orchestrates the entire process:\n",
    "- Sets up the Selenium WebDriver.\n",
    "- Loads progress if it exists, or scrapes the main page to get system URLs.\n",
    "- Iterates through each system URL, scrapes document links, and downloads the documents.\n",
    "- Saves progress after each download.\n",
    "- Handles exceptions to skip problematic URLs and continue with the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `Defining constants`\n",
    "\"\"\"\n",
    "    base_url: The base URL of the CANTEACH website.\n",
    "    main_page_url: The URL of the main page with the list of systems to scrape system urls from\n",
    "    save_folder: The local directory where the documents will be saved.\n",
    "    progress_file: The file where progress will be saved and loaded from.\n",
    "\"\"\"\n",
    "base_url = \"https://canteach.candu.org\"\n",
    "main_page_url = f\"{base_url}/SitePages/Publications%20by%20System.aspx\"\n",
    "save_folder = \"CANTEACH_Documents\"\n",
    "progress_file = \"progress.pkl\"\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that initializes the Selenium WebDriver for Firefox, with an option to run headless.\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Set up the Selenium WebDriver for Firefox.\n",
    "\n",
    "    Returns:\n",
    "        WebDriver: Configured Selenium WebDriver for Firefox.\n",
    "    \"\"\"\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    # options.add_argument(\"--headless\")  # Should be uncommented to run headless\n",
    "    driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()), options=options)\n",
    "    driver.delete_all_cookies()\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape the main page and get system URLs with their names\n",
    "def get_system_urls(main_page_url):\n",
    "    \"\"\"\n",
    "    Scrape the main page to get system URLs with their names.\n",
    "\n",
    "    Args:\n",
    "        main_page_url (str): URL of the main page listing systems.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with system names as keys and their corresponding URLs as values.\n",
    "    \"\"\"\n",
    "    response = requests.get(main_page_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    system_urls = {}\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if \"Forms\" in href:\n",
    "            system_name = link.text.strip()\n",
    "            system_urls[system_name] = base_url + href\n",
    "    \n",
    "    return system_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape a system page and get document links with Selenium\n",
    "def get_document_links(driver, system_url):\n",
    "    \"\"\"Scrape a system page to get document links using Selenium.\n",
    "\n",
    "    Args:\n",
    "        driver (WebDriver): The Selenium WebDriver instance.\n",
    "        system_url (str): URL of the system page to scrape.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of document URLs.\n",
    "    \"\"\"\n",
    "    driver.get(system_url)\n",
    "    driver.delete_all_cookies()  # Clear cookies again before each request to handle \"400 Bad request\" HTTP error\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.presence_of_all_elements_located((By.TAG_NAME, 'span')))\n",
    "\n",
    "    document_urls = set()  # Uses a set to avoid duplicate URLs\n",
    "\n",
    "    print(f\"Scraping page: {driver.current_url}\")\n",
    "\n",
    "    # Locate all 'Title' elements on the web-page using XPath\n",
    "    title_elements = driver.find_elements(By.XPATH, \"//a[contains(text(),'Title')]\")\n",
    "    \n",
    "    print(f\"Found {len(title_elements)} elements with 'Title'\")\n",
    "\n",
    "    for title_element in title_elements:\n",
    "        try:\n",
    "            # Scrolling the element into view using JavaScript\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", title_element)\n",
    "            time.sleep(1)  # Wait for scrolling to complete\n",
    "\n",
    "            # Using ActionChains to move to the element and click\n",
    "            ActionChains(driver).move_to_element(title_element).click().perform()\n",
    "            time.sleep(2)  # Wait for the PDFs to be displayed\n",
    "\n",
    "            # Finding all PDF links within the expanded section\n",
    "            pdf_links = driver.find_elements(By.CSS_SELECTOR, 'a.ms-listlink')\n",
    "            for link in pdf_links:\n",
    "                href = link.get_attribute('href')\n",
    "                if href and 'Content%20Library' in href and href.endswith('.pdf'):\n",
    "                    # Fixing the URL concatenation issue\n",
    "                    if href.startswith(\"/\"):\n",
    "                        href = base_url + href\n",
    "                    document_urls.add(href)  # Adding URL to set\n",
    "                    print(f\"Found PDF link: {href}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not click element. Error: {str(e)}\")\n",
    "\n",
    "    return document_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download a document and save it in the appropriate folder\n",
    "def download_document(document_url, save_folder):\n",
    "    \"\"\"Download a document and save it in the appropriate folder.\n",
    "\n",
    "    Args:\n",
    "        document_url (str): URL of the document to download.\n",
    "        save_folder (str): Path to the folder where the document will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(document_url)\n",
    "        response.raise_for_status()\n",
    "        document_name = document_url.split('/')[-1]\n",
    "        \n",
    "        save_path = os.path.join(save_folder, document_name)\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded: {document_name}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {document_url}. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save progress to a file\n",
    "def save_progress(system_urls, downloaded_urls):\n",
    "    \"\"\"Save progress to a file.\n",
    "\n",
    "    Args:\n",
    "        system_urls (dict): Dictionary of system URLs.\n",
    "        downloaded_urls (set): Set of downloaded document URLs.\n",
    "    \"\"\"\n",
    "    with open(progress_file, 'wb') as f:\n",
    "        pickle.dump((system_urls, downloaded_urls), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finction to load progress from a file\n",
    "def load_progress():\n",
    "    \"\"\"Load progress from a file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the dictionary of system URLs and the set of downloaded document URLs.\n",
    "    \"\"\"\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None, set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to orchestrate the process\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the process.\"\"\"\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    # Load progress if exists\n",
    "    saved_system_urls, downloaded_urls = load_progress()\n",
    "    \n",
    "    if saved_system_urls:\n",
    "        system_urls = saved_system_urls\n",
    "    else:\n",
    "        # Scrape the main page to get system URLs\n",
    "        system_urls = get_system_urls(main_page_url)\n",
    "\n",
    "    try:\n",
    "        # Print system URLs\n",
    "        print(\"System URLs:\")\n",
    "        for system_name, system_url in system_urls.items():\n",
    "            print(f\"{system_name}: {system_url}\")\n",
    "\n",
    "        # Iterate through each system URL to get document links and download them\n",
    "        for system_name, system_url in system_urls.items():\n",
    "            # Create system folder\n",
    "            system_folder = os.path.join(save_folder, system_name.replace(' ', '_'))\n",
    "            os.makedirs(system_folder, exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                document_links = get_document_links(driver, system_url)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to scrape documents from {system_url}. Error: {e}\")\n",
    "                continue  # Skip to the next system URL\n",
    "\n",
    "            # Print document URLs\n",
    "            print(f\"\\nDocument URLs for {system_name}:\")\n",
    "            for document_link in document_links:\n",
    "                if document_link not in downloaded_urls:\n",
    "                    print(document_link)\n",
    "                    download_document(document_link, system_folder)\n",
    "                    downloaded_urls.add(document_link)\n",
    "                    save_progress(system_urls, downloaded_urls)  # Save progress after each download\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
